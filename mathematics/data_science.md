# 밑바닥부터 시작하는 데이터 과학

## 분포

#### 기하분 
- 조건
  - 일련의 독립시행하고, 각 시행의 결과는 성공 아니면 실패.
  - 성공할 확률은 각 시행에서 동일하다.
  - **관심이 있는 부분이 첫번째 성공을 얻을 때까지 시도해야 하는 시행의 횟수인 경우에 성립한다.**
- 수식
  - 조건이 기하분포에 부합하고, X가 첫번째 성공을 얻을 때까지 시도해야 하는 횟수를 나타내고
  p는 한번 시행에서 성공할 확률을 의미한다면, 다음과 같이 표기합니다.
    ```
    X ~ Geo(p)
    ```
  

- 확률에 대한 다음 공식이 성립
  ```
  P(X = r) = pq^(r-1)
  P(X > r) = q^r
  P(X <= r) = 1 - q^r
  ```

- 만약 X ~ Geo(p)라면 다음 공식이 성립
  ```
  E(X) = 1/p
  Var(X) = q/p^2
  ```

#### 이항분포
- 조건
  - 일련의 독립시행을 하고, 각 시행의 결과는 성공아니면 실패.
  - 성공할 확률은 각 시행에서 동일하다.
  - **관심이 있는 부분이 n번의 독립시행을 실시했을 때 성공을 거두는 횟수인 경우에 성립합니다.**
- 수식
  - 조건이 이항분포에 부합하고, X가 n번의 시행에서 성공을 거둔 횟수를 나타내고, p는 한 번 시행에서 성공할 확률을 의미한다면, 다음과 같이 표기합니다.
    ```
    X ~ B(n, p)
    ```

  - 만약 X ~ B(n, p)이면 다음 공식을 이용해서 확률을 구할 수 있다.
    ```
    P(X = r) = nCr * p^r * q^(n-r)
    여기서 nCr = n! / r!(n - r)!
    ```

  - 만약 X ~ B(n, p)이면 다음 공식이 성립한다.
    ```
    E(X) = np
    Var(X) = npq
    ```

#### 푸아송분포
- 조건
  - 개별적인 사건이 어떤 주어진 구간 내에서 임의로 그리고 독립적으로 발생한다.
  - 구간 안에서 사건이 발생하는 수의 평균값이나 비율이 알려져 있고 유한하다.
  - **관심이 있는 부분이 구간 안에서 발생하는 사건의 수인 경우에 성립한다.**
- 수식
  - 만약 조건이 푸아송분포에 부합하고, X가 특정 구간 내에서 사건이 발생한 수를 나타내고, λ가 사건이 발생하는 비율이라고 한다면, 다음과 같이 표기한다.
    ```
    X ~ Po(λ)
    ```

  - 만약 X ~ Po(λ)라면 다음 공식이 성립한다.
    ```
    P(X = r) = (e^(-λ) * λ^(r)) / r!
    E(X) = λ
    Var(X) = λ
    ```

  - 만약 X ~ Po(λ x)이고 Y ~ Po(λ y)이며 X와 Y가 서로 독립이라면 다음과 같이 표기한다.
    ```
    X + Y ~ Po(λ x + λ y)
    ```

  - 만약 n이 크고 p가 작으며 X ~ B(n, p) 라면 X ~ Po(np)를 이용해서 그 값에 대한 근사치를 구할 수 있다.


## 활용
#### 정규분포를 이용해서 이항분포의 근사치를 구할 수 있다.
- 조건
  - X ~ B(n, p), np > 5, 그리고 nq > 5라면 X ~ N(np, npq)를 이용해서 이항분포에 대한 대략적인 값을 구할 수 있다.
  - 정규분포를 이용해서 이항분포에 대한 근사치를 구하는 경우에는 정확한 결과를 얻도록 하기 위해 연속성 보정(continuity correction)을 적용할 필요가 있다.
- 수식
  - X ~ B(n, p) 일 때, np > 5이고 nq > 5라면 정규분포를 이용
  - X ~ B(n, p 일 때, p < 0.1이라면 푸아송분포를 이용
  
#### 정규분포를 이용해서 푸아송분포의 근사치를 구할 수 있다.
λ의 값이 크면 푸아송분포의 모양은 점점 정규분포의 모양을 닮아간다.
- 조건 
  - X ~ Po(λ)이고 λ > 15이면 X ~ N(λ, λ)를 이용해서 근사치를 구할 수 있다.
  
  - 정규분포를 이용해서 푸아송분포의 근사치를 구한다면 정확한 결과를 얻기 위해 반드시 연속성보정을 적용해 주어야 한다.
    ```
    연속성 보정(Continuity correction)
    
    정수형 변수 X에 대한 확률을 연속성 분포로 근사하는 과정에서 적용되는 보정기법을 말한다.
    주로 다음 등식을 이용한다.
    - 찾는 대상이 확률사이에 있는 경우 
      {a <= X <= b} = {a - 0.5 <= X <= b + 0.5}
    - 찾는 대상이 확률보다 작은경우
      { X <= a } = {X <= a + 0.5}
    - 찾는 대상이 확률보다 큰 경우
      { X >= b } = {X >= b - 0.5}
    ```
  - 관련링크
    - [연속성 수정](http://terms.naver.com/entry.nhn?docId=3405223&cid=47324&categoryId=47324)



## 경사 하강법

#### 왜 해야 하나?
- 데이터 과학을 하다보면 특정 상황에 가장 적합한 모델을 찾아야 할 때가 많다.
- 가장 적합한? "모델의 오류를 최소화 하는" 또는 "likelihood(우도)를 최대한 하는"을 의미한다.
- 밑바닥에서 부터 구현하기도 용이하다.

#### 경사 하강법의 숨은 의미
주어진 함수 f를 종종 최대화(또는 최소화)해야 한다.
즉, 함수 f를 최대화 시키는 입력값 v를 찾아야 한다.
gradient는 함수가 가장 빠르게 증가할 수 있는 방향을 나타낸다.  

- 방법
  - 임의의 시작점을 잡은 후, gradient를 계산한다.
  - gradient의 방향으로 조금 이동하는 과정을 여러번 반복한다.
  - 정방향 그리고 반대방향으로 이동함에 따라 최대값, 최솟값을 구할 수 있다.


#### Gradient 계산하기
python에서는 극한값을 구할 수 없기 때문에, 아주 작은 e값을 대입해 미분값을 어림잡아서 계산한다.

#### Gradient 적용하기
**임의의 시작점을 잡고 경사의 반대 방향으로 조금씩 이동한다.**

#### 적절한 이동 거리 정하기

- 방법
  * 방법1. 이동 거리를 고정
  * 방법2. 시간에 따라 이동 거리를 점차 줄임
  * 방법3. 이동할 때마다 목적 함수를 최소화하는 이동거리로 정함

- 다른방법
위의 "방법3"이 가장 좋아보이나 계산비용이 너무 크다. 대신 몇몇 정해진 이동 거리를 시도해보고 그중에서 목적 함수를 가장 최소화하는 값을 고르는 방법도 있다.



## 데이터 다루기
#### 데이터 정제
* 나쁜데이터 처리
  - 나쁜 데이터 제거
  - 나쁜 데이터 직접 수정
  - 나쁜 데이터 없기를 기도한다.
* 이상치 확인

#### 데이터 처리

#### 척도 조절
각 차원의 수치가 크게 다른 경우, 각 차원의 평균을 0, 표준편차를 1로 변환시키면서 척도를 조절해 줄 수 있다.

#### 차원 축소
차원 축소는 다음과 같은 이유 때문에 중요하다.
  * 차원 축소는 잡음에 해당하는 차원을 제거해 주고 밀접한 연관된 차원을 합쳐 주면서 데이터를 정제해 준다.
  * 저차원으로 축소시킨 데이터에서는 고차원 데이터에서는 사용할 수 없었던 다양한 기법을 사용할 수 있게 된다.

주의! 차원 축소를 통해 더 좋은 성능의 모형을 만들 수 있다고는 해도, 만들어진 모형을 해석하는 것은 어려워진다.



## 기계학습
#### 모델링
- 모델이란?  
  ```
  다양한 변수 간의 수학적(혹은 확률적) 관계를 표현한 것이다.
  ```

#### 기계학습이란?
사람마다 기계학습에 대한 정의는 다르다.
교제에서는 기계학습을 데이터를 통해 모델을 만들고 사용하는 것이라고 정의한다.
예측 모델링(predictive modeling) 또는 데이터마이닝(data mining)이라 불릴 수도 있다.

#### 오버피팅과 언더피팅
- 오버피팅(overfitting)
  - 기계학습의 일반적인 문제점
  - 만들어진 모델의 성능이 학습 데이터에서는 좋지만, 기존에 관측한 적이 없는 새로운 데이터에서는 좋지 않은 경우를 말한다.
    - 왜 발생하는가?
      * 데이터 잡음까지 모델에 학습되는 경우
      * 원하는 결과를 예측해주는 요소가 아닌 다른요소들이 학습되는 경우

- 언더피팅(underfitting)
  - 모델의 성능이 학습 데이터에서도 좋지 않은 경우
  - 이 경우 새로운 모델을 찾아보자.      

- 해결안
  - 데이터를 세 종류의 데이터로 나눠서 사용할 수 있다. 
    * 학습 데이터로 모델을 만든다.
    * 검증 데이터(validation set)를 통해 여러 모델중 하나를 선택한다.
    * 평가 데이터로 최종 모델의 성능을 평가한다.

#### 정확도
모델의 성능을 평가하기 위해 정밀도와 재현율을 사용한다.
  - 정밀도 : 양성으로 예측된 결과의 정확도
  - 재현율 : 실제 양성 중 모델이 정확하게 양성으로 예측한 비율

#### Bias-variance 트레이드오프
오버피팅 문제는 bias(편향)와 variance(분산)의 트레이드오프로 볼 수도 있다.

#### 특성 추출 및 선택
데이터의 특성을 나타내는 모델의 변수가 부족하다면 언더피팅이 발생할 것이다.
하지만 변수가 너무 많다면 오버피팅이 발생할 것이다.



## k-NN

#### 모델
k-NN(k-Nearest Neighbors, k-근접이웃)은 가장 단순한 예측 모델 중 하나이다.
오직 필요한 것은 다음과 같다.
  - 거리를 재는 방법
  - 서로 가까운 점들은 유사하다는 가정

#### 차원의 저주
- 차원이 높아질 수록 점들 사이의 거리가 멀어지고 빈 공간이 많아진다.
- 데이터가 기하급수적으로 많지 않다면, 고차원에서는 모든 점 사이의 거리가 멀어진다.
- 따라서 고차원에서 k-NN을 이용하려면, 먼저 차원 축소를 하는 것이 좋을 것이다.



## 나이브 베이즈(Naïve Bayes)

#### 바보 스팸 필터

#### 조금 더 똑똑한 스팸필터
- 나이브 베이즈의 핵심
  ```
  '메시지가 스팸이냐 아니냐가 주어졌다는 조건하에 각 단어의 존재(혹은 부재)는 서로 조건부 독립적이다'라는 가정에 기반을 둔다.
  ```

- smoothing(평활화)가 필요하다.
Smoothing을 위해 가짜 빈도수(pseudocount) k를 결정하고 스팸 메시지에서 i번째 단어가 나올 확률을 다음과 같이 추정할 수 있다.
  ```
  P(Xi|S)=(k + wi를 포함하고 있는 스팸 수)/(2k + 스팸 수)
  ```

#### 구현하기

#### 모델 검증하기


## 단순 회귀 분석

#### 모델
전제가 선형일경우 당연히 선형의 수식으로 설명할 수 있다.
```
y_i = bx_i + a + e_i
```

최소자승법을 사용하여 오류값을 보다 정확하게 측정할 수 있다.
미분을 최소화하여 오류를 최소화하는 알파와 베타를 찾을 수 있다.

- 결정계수
모델이 주어진 데이터에 얼마나 적합한지 알아보기 위해 그래프를 살펴보는 것보다 더 좋은 방법이 필요하다.
보통 결정계수(R 제곱 값)라는 수치를 사용한다.
결정계수란 종족 변수의 총 변화량 중 모델이 잡아낼 수 있는 변화량의 비율을 의미한다.

#### 경사 하강법 사용하기

#### 최대우도추정법
최소자승법을 사용한 이유는 무엇인가? 
최대우도추정법(Maximum likelihood estimation, MLE)이기 때문이다.
최대가능도방법이라고도 불린다.
https://ko.wikipedia.org/wiki/%EC%B5%9C%EB%8C%80%EA%B0%80%EB%8A%A5%EB%8F%84_%EB%B0%A9%EB%B2%95



## 다중 회귀 분석

#### 모델

#### 최소자승법에 대한 몇 가지 추가 가정

#### 모델 학습하기

#### 모델 해석하기

#### 적합성(Goodness of fit)

#### Regularization
변수가 굉장히 많은 데이터에 회귀 분석 모델을 적용해야 하는 경우가 자주 발생한다.
이 경우 다음의 문제가 발생할 수 있다.
  * 모델이 학습 데이터에 오버피팅 될 수 있다.
  * 0이 아닌 계수가 많을 수록 모델을 해석하기 어려워진다.

Regularization은 beta가 커지면 커질수록 해당 모델에게 패널티를 주는 방법이다.
그리고 오류와 패널티를 최소화하는 최적의 모델을 만들 수 있다.

- 적용사례
  - 리지 회귀(ridge regression)
  - 라쏘 회귀(lasso regression)  



## 로지스틱 회귀 분석

#### 기존 회귀 분석의 문제점
* 선형 회귀 모델의 경우 해석이 어려워 질 수 있다.
* 선형 회귀 분석은 오류값(error)이 변수들과 아무런 상관 관계(correlation)가 없다고 가정한다.

#### 로지스틱 함수
로지스틱 회귀 분석은 로지스틱 함수를 사용한다.
로지스틱 함수는 입력값이 커질수록 출력값이 1에 가까워진다.
반대로 작아질 수록 0에 가까워진다.

- 미분한 로지스틱 함수
  ```
  def logistic_prime(x):
      return logistic(x) * (1 - logistic(x))
  ```

#### 모델 적용하기

#### 적합성(Goodness of fit)

#### SVM



## 의사결정나무

#### 의사결정나무
다양한 의사결정 경로(decision path)와 결과(outcome)를 나타내는 데 나무 구조를 사용한다.

- 장점
  - 이해와 해석이 쉽다.
  - 예측할 때 사용하는 프로세스가 꽤 명확하다.
  - 숫자형 데이터와 범주형 데이터를 동시에 다를 수 있다.
  - 특정 변수의 값이 누락되어도 사용할 수 있다.

- 단점
  - '최적의' 의사결정나무를 찾는 것은 계산적으로 무척 어렵다.
  - 새로운 데이터에 대한 일반화 성능이 좋지 않게 오버피팅되기 쉽다.
  
- 종류
  - 범주형 결과를 반환하는 분류나무(classification tree)
  - 숫자형 결과를 반환하는 회귀나무(regression tree)

#### 엔트로피
- 엔트로피?
  ```
  '얼마만큼의 정보를 담고 있는가'를 엔트로피(entropy)라고 한다.
  엔트로피는 '무질서도(disorder)'를 의미한다는 말도 있다.
  데이터의 불확실성(uncertainty)을 나타낼 때도 같은 표현을 사용한다.
  ```

- 의사결정 나무를 만들기 위한 방법
  * 먼저 어떤 질문을 물을 것인지 정해야 한다.
  * 어떤 순서로 질문을 던질 것인지 정해야 한다.

#### 의사결정나무 만들기
의사결정나무는 결정 노드(decision node)와 잎 노드(leaf node)로 구성된다.
ID3알고리즘에 기반한 의사결정나무를 만들면 'greedy(탐욕적)'알고리즘으로 만들어진다.
ID3알고리즘으로 만들게 되면 이해가 쉽고 구현하기 쉬워서 입문에 좋은 길잡이가 된다.

#### 종합하기

#### 랜덤포레스트
오버피팅을 방지할 수 있는 대표적인 방법중의 하나인 '랜덤포레스트(random forest)'.
이는 여러 개의 의사결정나무를 만들고, 그들의 다수결로 결과를 결정하는 방법이다.
아주 인기가 많고 두루 쓰이는 알고리즘 중 하나이다.

- 방법1. bootstrap aggregating(부트스트랩 통합) 또는 bagging이라고 한다.
  - 데이터를 bootstrap한다. 
  - 전체 데이터를 inputs로 학습하는 것이 아니라 bootstrap_sample(inputs)의 결과물을 각 나무의 입력값으로 넣어 학습하는 것이다.
- 방법2. 파티션을 나누는 변수에 랜덤성을 부여한다.
  - 좀 더 광범위하게 얘기하면 '앙상블 학습(ensemble learning)'이라고 한다.
  - 성능이 떨어지는 (그리고 대부분 bias 가 높고 varianc가 낮은) 여러 모델을 동시에 활용해서 전체적으로는 성능이 좋은 모델을 구축한다.


## 신경망
인공신경망(artificial neural network)은 뇌를 묘사한 예측 모델이다.

- 뉴런의 동작 방식
  - 각 뉴런은 다른 뉴런이 출력한 결과를 입력 받아 특정 연산을 수행한다.
  - 계산 결과가 특정 임계치를 넘으면 활성화되고 넘지 않으면 활성화되지 않는다.

초보 데이터 과학자가 문제를 해결하려 할 때 활용하기에는 좋은 방법은 아니다.
하지만 언젠가 인공지능(artificial intelligence)을 만들어 기술적 특이점을 앞당기고자 할 때 좋은 선택이 될 수 있다.

#### 퍼셉트론
n개의 이진수가 하나의 뉴런을 통과해서 가중합이 0보다 크면 활성화되는 가장 간단한 신경망 구조이다.
  ```
  def step_function(x):
      return 1 if x >= 0 else 0

  def perceptron_output(weights, bias, x):
      calculation = dot(weights, x) + bias
      return step_function(calculation)
  ```

퍼셉트론은 초평면(hyperplane)으로 구분되는 두 개의 공간을 분리시키는 역할을 한다.
  ```
  dot(weights, x) + bias == 0
  ```

#### 순방향(Feed-forward) 신경망

- 일반적이 묘사
  - 입력층(input layer) : 입력값을 받고 그대로 다음 층으로 값을 전송한다.
  - 은닉층(hidden layer) : 직전층의 출력값을 입력 받아 어떤 계산을 하고 다음 층으로 전달하는 역할을 한다.
  - 출력층(output layer) : 최종값을 반환.

#### Backpropagation
신경망을 학습시키는 방법중에 하나이다.
경사 하강법과 유사한 점이 상당히 많다.

#### 예시: CAPTCHA 깨기
단순한 CAPTCHA는 어렵지 않게 가능하다.


## 군집화(Clustering)
군집화는 레이블이 없는 데이터를 이용하는, 또는 레이블이 있더라도 그것을 사용하지 않는 비지도 학습(unsupervised learning)의 일종이다.

#### 군집화 감 잡기
대부분의 데이터는 어떤 방식으로는 군집을 이루기 마련이다.

#### 모델
가장 간단한 군집화 방법중의 하나는 군집의 개수 k를 미리 정해 두는 k-means이다.
  ```
  k-means

  데이터와 데이터가 속한 군집의 중심점과의 거리의 제곱합을 최소화시키며 S_1, ..., S_k까지의 군집을 구한다.
  ```

#### 예시 : 오프라인 모임

#### k값 선택하기
쉬운 방법중 하나는 k값에 대해 중심점과 각 데이터 포인트 사이의 거리의 제곱합을 그래프로 그리고, 그 그래프가 어디서 꺾이는지 관찰하는 것이다.

#### 예시: 색 군집화하기

#### 상향식 계층 군집화
- 방법
  * 각 데이터 포인트를 하나의 군집으로 간주한다.
  * 군집이 두 개 이상이라면, 가장 가까운 두 개의 군집을 찾아 하나의 군집으로 묶는다.

위의 반복 연산이 완전히 종료되면, 최종적으로 단 하나의 거대한 군집만이 남게된다.
다만, 군집들을 묶어 나가는 과정을 모두 기록해 두기 때문에, 언제든 묶인 군집을 다시 풀 수 있다.


## 자연어 처리
자연어 처리(NLP, natural language processing)는 언어에 대한 계산적 기술(computational techniques)의 집합이다.

#### 워드 클라우드
단어의 빈도수가 폰트의 크기를 의미하게 하고, 좀 더 의미있는 워드 클라우드를 보이기 위해 xy축에 의미를 부여하고, 색을 입히는 방법도 있다.

#### n-gram 모델
간단히 말하자면 입력한 문자열을 N개의 기준 단위로 절단하는 방법이다.

#### 문법


#### 여담: 깁스 샘플링(Gibbs sampling)
균등분포와 정규분포에서 샘플링하는 것은 쉽다.
하지만 다른 복잡한 확률분포로부터 데이터를 샘플링하는 것은 간단한 작업이 아니다.
깁스 샘플링은 이때 사용할 수 있는 기술 중 하나이다.

  ```
  깁스 샘플링

  임의의 (유효한) x또는 y값에서 출발해서 x에 대한 y의 조건부 확률과 y에 대한 x의 조건부 확률 사이를 오가며 반복적으로 값을 선택하는 방법이다.
  이 과정을 여러 번 반복하여 얻은 x와 y는 결합확률분포(joint distribution)에서 얻은 샘플이라고 볼 수 있다.
  ```

#### 토픽 모델링



## 네트워크 분석
많은 데이터 문제는 노드(node)와 그 사이를 연결하는 엣지(edge)로 구성된 네트워크(network)의 관점에서 볼 수 있다.

#### 매개 중심성(betweenness centrality)

#### 고유벡터 중심성

#### 방향성 그래프(Directed graphs)와 페이지랭크
- 페이지랭크의 기본철학
  ```
  예를들어 보증을 많이 받은 사용자가 보증을 설 때는, 보증을 적게 받은 사용자가 보증을 설 때보다 더 중요힌 것으로 받아들여지는 것이 타당하다.
  ```



## 추천 시스템

#### 수작업을 이용한 추천

#### 인기도를 활용한 추천

#### 사용자 기반 협업 필터링
- 코사인 유사도(cosine similarity)
  - 사용자들 간 유사도의 정의 방법
  - 두 벡터 v, w가 주어졌을 때, 벡터 v, w 사이의 '각도'를 잰다. 
  - v, w가 완전히 같은 방향을 가리키고 있다면, 분모와 분자가 같은 값을 갖게 되어 코사인 유사도 값은 1이 된다.


#### 상품 기반 협업 필터링
