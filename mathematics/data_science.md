# 밑바닥부터 시작하는 데이터 과학

## 분포

#### 기하분포
- 조건
  - 일련의 독립시행하고, 각 시행의 결과는 성공 아니면 실패.
  - 성공할 확률은 각 시행에서 동일하다.
  - **관심이 있는 부분이 첫번째 성공을 얻을 때까지 시도해야 하는 시행의 횟수인 경우에 성립한다.**
- 수식
  - 조건이 기하분포에 부합하고, X가 첫번째 성공을 얻을 때까지 시도해야 하는 횟수를 나타내고
  p는 한번 시행에서 성공할 확률을 의미한다면, 다음과 같이 표기합니다.
    ```
    X ~ Geo(p)
    ```
  

- 확률에 대한 다음 공식이 성립
  ```
  P(X = r) = pq^(r-1)
  P(X > r) = q^r
  P(X <= r) = 1 - q^r
  ```

- 만약 X ~ Geo(p)라면 다음 공식이 성립
  ```
  E(X) = 1/p
  Var(X) = q/p^2
  ```

#### 이항분포
- 조건
  - 일련의 독립시행을 하고, 각 시행의 결과는 성공아니면 실패.
  - 성공할 확률은 각 시행에서 동일하다.
  - **관심이 있는 부분이 n번의 독립시행을 실시했을 때 성공을 거두는 횟수인 경우에 성립합니다.**
- 수식
  - 조건이 이항분포에 부합하고, X가 n번의 시행에서 성공을 거둔 횟수를 나타내고, p는 한 번 시행에서 성공할 확률을 의미한다면, 다음과 같이 표기합니다.
    ```
    X ~ B(n, p)
    ```

  - 만약 X ~ B(n, p)이면 다음 공식을 이용해서 확률을 구할 수 있다.
    ```
    P(X = r) = nCr * p^r * q^(n-r)
    여기서 nCr = n! / r!(n - r)!
    ```

  - 만약 X ~ B(n, p)이면 다음 공식이 성립한다.
    ```
    E(X) = np
    Var(X) = npq
    ```

#### 푸아송분포
- 조건
  - 개별적인 사건이 어떤 주어진 구간 내에서 임의로 그리고 독립적으로 발생한다.
  - 구간 안에서 사건이 발생하는 수의 평균값이나 비율이 알려져 있고 유한하다.
  - **관심이 있는 부분이 구간 안에서 발생하는 사건의 수인 경우에 성립한다.**
- 수식
  - 만약 조건이 푸아송분포에 부합하고, X가 특정 구간 내에서 사건이 발생한 수를 나타내고, λ가 사건이 발생하는 비율이라고 한다면, 다음과 같이 표기한다.
    ```
    X ~ Po(λ)
    ```

  - 만약 X ~ Po(λ)라면 다음 공식이 성립한다.
    ```
    P(X = r) = (e^(-λ) * λ^(r)) / r!
    E(X) = λ
    Var(X) = λ
    ```

  - 만약 X ~ Po(λ x)이고 Y ~ Po(λ y)이며 X와 Y가 서로 독립이라면 다음과 같이 표기한다.
    ```
    X + Y ~ Po(λ x + λ y)
    ```

  - 만약 n이 크고 p가 작으며 X ~ B(n, p) 라면 X ~ Po(np)를 이용해서 그 값에 대한 근사치를 구할 수 있다.


## 활용
#### 정규분포를 이용해서 이항분포의 근사치를 구할 수 있다.
- 조건
  - X ~ B(n, p), np > 5, 그리고 nq > 5라면 X ~ N(np, npq)를 이용해서 이항분포에 대한 대략적인 값을 구할 수 있다.
  - 정규분포를 이용해서 이항분포에 대한 근사치를 구하는 경우에는 정확한 결과를 얻도록 하기 위해 연속성 보정(continuity correction)을 적용할 필요가 있다.
- 수식
  - X ~ B(n, p) 일 때, np > 5이고 nq > 5라면 정규분포를 이용
  - X ~ B(n, p 일 때, p < 0.1이라면 푸아송분포를 이용
  
#### 정규분포를 이용해서 푸아송분포의 근사치를 구할 수 있다.
λ의 값이 크면 푸아송분포의 모양은 점점 정규분포의 모양을 닮아간다.
- 조건 
  - X ~ Po(λ)이고 λ > 15이면 X ~ N(λ, λ)를 이용해서 근사치를 구할 수 있다.
  
  - 정규분포를 이용해서 푸아송분포의 근사치를 구한다면 정확한 결과를 얻기 위해 반드시 연속성보정을 적용해 주어야 한다.
    ```
    연속성 보정(Continuity correction)
    
    정수형 변수 X에 대한 확률을 연속성 분포로 근사하는 과정에서 적용되는 보정기법을 말한다.
    주로 다음 등식을 이용한다.
    - 찾는 대상이 확률사이에 있는 경우 
      {a <= X <= b} = {a - 0.5 <= X <= b + 0.5}
    - 찾는 대상이 확률보다 작은경우
      { X <= a } = {X <= a + 0.5}
    - 찾는 대상이 확률보다 큰 경우
      { X >= b } = {X >= b - 0.5}
    ```
  - 관련링크
    - [연속성 수정](http://terms.naver.com/entry.nhn?docId=3405223&cid=47324&categoryId=47324)



## 경사 하강법

#### 왜 해야 하나?
- 데이터 과학을 하다보면 특정 상황에 가장 적합한 모델을 찾아야 할 때가 많다.
- 가장 적합한? "모델의 오류를 최소화 하는" 또는 "likelihood(우도)를 최대한 하는"을 의미한다.
- 밑바닥에서 부터 구현하기도 용이하다.

#### 경사 하강법의 숨은 의미
주어진 함수 f를 종종 최대화(또는 최소화)해야 한다.
즉, 함수 f를 최대화 시키는 입력값 v를 찾아야 한다.
gradient는 함수가 가장 빠르게 증가할 수 있는 방향을 나타낸다.  

- 방법
  - 임의의 시작점을 잡은 후, gradient를 계산한다.
  - gradient의 방향으로 조금 이동하는 과정을 여러번 반복한다.
  - 정방향 그리고 반대방향으로 이동함에 따라 최대값, 최솟값을 구할 수 있다.


#### Gradient 계산하기
python에서는 극한값을 구할 수 없기 때문에, 아주 작은 e값을 대입해 미분값을 어림잡아서 계산한다.

#### Gradient 적용하기
**임의의 시작점을 잡고 경사의 반대 방향으로 조금씩 이동한다.**

#### 적절한 이동 거리 정하기

- 방법
  * 방법1. 이동 거리를 고정
  * 방법2. 시간에 따라 이동 거리를 점차 줄임
  * 방법3. 이동할 때마다 목적 함수를 최소화하는 이동거리로 정함

- 다른방법
위의 "방법3"이 가장 좋아보이나 계산비용이 너무 크다. 대신 몇몇 정해진 이동 거리를 시도해보고 그중에서 목적 함수를 가장 최소화하는 값을 고르는 방법도 있다.



## 데이터 다루기
#### 데이터 정제
* 나쁜데이터 처리
  - 나쁜 데이터 제거
  - 나쁜 데이터 직접 수정
  - 나쁜 데이터 없기를 기도한다.
* 이상치 확인

#### 데이터 처리

#### 척도 조절
각 차원의 수치가 크게 다른 경우, 각 차원의 평균을 0, 표준편차를 1로 변환시키면서 척도를 조절해 줄 수 있다.

#### 차원 축소
차원 축소는 다음과 같은 이유 때문에 중요하다.
  * 차원 축소는 잡음에 해당하는 차원을 제거해 주고 밀접한 연관된 차원을 합쳐 주면서 데이터를 정제해 준다.
  * 저차원으로 축소시킨 데이터에서는 고차원 데이터에서는 사용할 수 없었던 다양한 기법을 사용할 수 있게 된다.

주의! 차원 축소를 통해 더 좋은 성능의 모형을 만들 수 있다고는 해도, 만들어진 모형을 해석하는 것은 어려워진다.



## 기계학습
#### 모델링
- 모델이란?  
  ```
  다양한 변수 간의 수학적(혹은 확률적) 관계를 표현한 것이다.
  ```

#### 기계학습이란?
사람마다 기계학습에 대한 정의는 다르다.
교제에서는 기계학습을 데이터를 통해 모델을 만들고 사용하는 것이라고 정의한다.
예측 모델링(predictive modeling) 또는 데이터마이닝(data mining)이라 불릴 수도 있다.

#### 오버피팅과 언더피팅
- 오버피팅(overfitting)
  - 기계학습의 일반적인 문제점
  - 만들어진 모델의 성능이 학습 데이터에서는 좋지만, 기존에 관측한 적이 없는 새로운 데이터에서는 좋지 않은 경우를 말한다.
    - 왜 발생하는가?
      * 데이터 잡음까지 모델에 학습되는 경우
      * 원하는 결과를 예측해주는 요소가 아닌 다른요소들이 학습되는 경우

- 언더피팅(underfitting)
  - 모델의 성능이 학습 데이터에서도 좋지 않은 경우
  - 이 경우 새로운 모델을 찾아보자.      

- 해결안
  - 데이터를 세 종류의 데이터로 나눠서 사용할 수 있다. 
    * 학습 데이터로 모델을 만든다.
    * 검증 데이터(validation set)를 통해 여러 모델중 하나를 선택한다.
    * 평가 데이터로 최종 모델의 성능을 평가한다.

#### 정확도
모델의 성능을 평가하기 위해 정밀도와 재현율을 사용한다.
  - 정밀도 : 양성으로 예측된 결과의 정확도
  - 재현율 : 실제 양성 중 모델이 정확하게 양성으로 예측한 비율

#### Bias-variance 트레이드오프
오버피팅 문제는 bias(편향)와 variance(분산)의 트레이드오프로 볼 수도 있다.

#### 특성 추출 및 선택
데이터의 특성을 나타내는 모델의 변수가 부족하다면 언더피팅이 발생할 것이다.
하지만 변수가 너무 많다면 오버피팅이 발생할 것이다.



## k-NN

#### 모델
k-NN(k-Nearest Neighbors, k-근접이웃)은 가장 단순한 예측 모델 중 하나이다.
오직 필요한 것은 다음과 같다.
  - 거리를 재는 방법
  - 서로 가까운 점들은 유사하다는 가정

#### 차원의 저주
- 차원이 높아질 수록 점들 사이의 거리가 멀어지고 빈 공간이 많아진다.
- 데이터가 기하급수적으로 많지 않다면, 고차원에서는 모든 점 사이의 거리가 멀어진다.
- 따라서 고차원에서 k-NN을 이용하려면, 먼저 차원 축소를 하는 것이 좋을 것이다.



## 나이브 베이즈(Naïve Bayes)

#### 바보 스팸 필터

#### 조금 더 똑똑한 스팸필터
- 나이브 베이즈의 핵심
  ```
  '메시지가 스팸이냐 아니냐가 주어졌다는 조건하에 각 단어의 존재(혹은 부재)는 서로 조건부 독립적이다'라는 가정에 기반을 둔다.
  ```

- smoothing(평활화)가 필요하다.
Smoothing을 위해 가짜 빈도수(pseudocount) k를 결정하고 스팸 메시지에서 i번째 단어가 나올 확률을 다음과 같이 추정할 수 있다.
  ```
  P(Xi|S)=(k + wi를 포함하고 있는 스팸 수)/(2k + 스팸 수)
  ```

#### 구현하기

#### 모델 검증하기



## 단순 회귀 분석

#### 모델


